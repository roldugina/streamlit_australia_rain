{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f23e055-9dcf-44d3-9894-ec71b901cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "from load_and_preprocess import load_model_components, preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506fea0d-e585-4ba2-aba2-a47740371d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model components\n",
    "model_components = load_model_components('models/aussie_rain.joblib')\n",
    "#joblib.dump(model, 'models/aussie_rain.joblib', compress=('zlib', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa44570-62d3-4a5c-8f32-aeba480f0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_df = pd.read_csv('data/weatherAUS.csv')\n",
    "\n",
    "raw_df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4415d113-b0f8-4db2-9f15-25e8a56345d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "X = raw_df[model_components['input_cols']]\n",
    "y = raw_df[model_components['target_col']]\n",
    "train_inputs, test_inputs, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51ae6a5-eee6-44f7-9303-a2f8a9de6eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location          0.00%\n",
       "MinTemp           0.32%\n",
       "MaxTemp           0.20%\n",
       "Rainfall          0.00%\n",
       "Evaporation      42.45%\n",
       "Sunshine         47.55%\n",
       "WindGustDir       6.51%\n",
       "WindGustSpeed     6.47%\n",
       "WindDir9am        6.85%\n",
       "WindDir3pm        2.60%\n",
       "WindSpeed9am      0.74%\n",
       "WindSpeed3pm      1.79%\n",
       "Humidity9am       1.07%\n",
       "Humidity3pm       2.50%\n",
       "Pressure9am       9.80%\n",
       "Pressure3pm       9.83%\n",
       "Cloud9am         37.38%\n",
       "Cloud3pm         39.84%\n",
       "Temp9am           0.45%\n",
       "Temp3pm           1.87%\n",
       "RainToday         0.00%\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check train dataset for null values\n",
    "train_inputs.isna().sum().apply(lambda x: format(x/train_inputs.shape[0],'.2%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c6df53-69d7-49ba-8d9d-8bbbadeed480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n",
      "C:\\study\\07_ML_course\\tasks\\streamlit\\load_and_preprocess.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[components['encoded_cols']] = components['encoder'].transform(data[components['categorical_cols']])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess raw data with trained imputer, scaler, encoder\n",
    "X_train, train_inputs = preprocess_data(train_inputs, model_components)\n",
    "X_test, test_inputs = preprocess_data(test_inputs, model_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33846fe-465c-44cb-b8be-4da01241cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No     0.8333    0.9811    0.9012     87668\n",
      "         Yes     0.8242    0.3109    0.4515     24961\n",
      "\n",
      "    accuracy                         0.8326    112629\n",
      "   macro avg     0.8288    0.6460    0.6763    112629\n",
      "weighted avg     0.8313    0.8326    0.8015    112629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define and train Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=40, max_leaf_nodes=30, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on train set\n",
    "pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, pred_train, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8bf281-390b-40fd-86bf-02895fe667de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No     0.8344    0.9798    0.9013     21918\n",
      "         Yes     0.8171    0.3171    0.4569      6240\n",
      "\n",
      "    accuracy                         0.8329     28158\n",
      "   macro avg     0.8258    0.6485    0.6791     28158\n",
      "weighted avg     0.8306    0.8329    0.8028     28158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05bd2d24-03db-43e2-931e-cbd868755141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/rf_model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model with new trained estimator\n",
    "rf_model = {\n",
    "    'model': model_components['model'],\n",
    "    'imputer': model_components['imputer'],\n",
    "    'scaler': model_components['scaler'],\n",
    "    'encoder': model_components['encoder'],\n",
    "    'input_cols': model_components['input_cols'],\n",
    "    'target_col': model_components['target_col'],\n",
    "    'numeric_cols': model_components['numeric_cols'],\n",
    "    'categorical_cols': model_components['categorical_cols'],\n",
    "    'encoded_cols': model_components['encoded_cols']\n",
    "}\n",
    "joblib.dump(rf_model, \"models/rf_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f3e4a1-e172-4215-aa29-b5107d15c562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140319.000000</td>\n",
       "      <td>140480.00000</td>\n",
       "      <td>140787.000000</td>\n",
       "      <td>81093.000000</td>\n",
       "      <td>73982.000000</td>\n",
       "      <td>131682.000000</td>\n",
       "      <td>139732.000000</td>\n",
       "      <td>138256.000000</td>\n",
       "      <td>139270.000000</td>\n",
       "      <td>137286.000000</td>\n",
       "      <td>127044.000000</td>\n",
       "      <td>127018.000000</td>\n",
       "      <td>88162.000000</td>\n",
       "      <td>84693.000000</td>\n",
       "      <td>140131.000000</td>\n",
       "      <td>138163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.184824</td>\n",
       "      <td>23.23512</td>\n",
       "      <td>2.349974</td>\n",
       "      <td>5.472516</td>\n",
       "      <td>7.630540</td>\n",
       "      <td>39.970520</td>\n",
       "      <td>13.990496</td>\n",
       "      <td>18.631141</td>\n",
       "      <td>68.826833</td>\n",
       "      <td>51.449288</td>\n",
       "      <td>1017.654577</td>\n",
       "      <td>1015.257963</td>\n",
       "      <td>4.431161</td>\n",
       "      <td>4.499250</td>\n",
       "      <td>16.987066</td>\n",
       "      <td>21.693183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.403879</td>\n",
       "      <td>7.11450</td>\n",
       "      <td>8.465173</td>\n",
       "      <td>4.189132</td>\n",
       "      <td>3.781729</td>\n",
       "      <td>13.578201</td>\n",
       "      <td>8.886210</td>\n",
       "      <td>8.798096</td>\n",
       "      <td>19.063650</td>\n",
       "      <td>20.807310</td>\n",
       "      <td>7.104867</td>\n",
       "      <td>7.035411</td>\n",
       "      <td>2.886594</td>\n",
       "      <td>2.719752</td>\n",
       "      <td>6.496012</td>\n",
       "      <td>6.937784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.500000</td>\n",
       "      <td>-4.80000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980.500000</td>\n",
       "      <td>977.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.200000</td>\n",
       "      <td>-5.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.600000</td>\n",
       "      <td>17.90000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>1010.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>16.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.60000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1017.600000</td>\n",
       "      <td>1015.200000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>21.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.800000</td>\n",
       "      <td>28.30000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1022.400000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>26.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.900000</td>\n",
       "      <td>48.10000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1041.000000</td>\n",
       "      <td>1039.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>40.200000</td>\n",
       "      <td>46.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MinTemp       MaxTemp       Rainfall   Evaporation      Sunshine  \\\n",
       "count  140319.000000  140480.00000  140787.000000  81093.000000  73982.000000   \n",
       "mean       12.184824      23.23512       2.349974      5.472516      7.630540   \n",
       "std         6.403879       7.11450       8.465173      4.189132      3.781729   \n",
       "min        -8.500000      -4.80000       0.000000      0.000000      0.000000   \n",
       "25%         7.600000      17.90000       0.000000      2.600000      4.900000   \n",
       "50%        12.000000      22.60000       0.000000      4.800000      8.500000   \n",
       "75%        16.800000      28.30000       0.800000      7.400000     10.700000   \n",
       "max        33.900000      48.10000     371.000000    145.000000     14.500000   \n",
       "\n",
       "       WindGustSpeed   WindSpeed9am   WindSpeed3pm    Humidity9am  \\\n",
       "count  131682.000000  139732.000000  138256.000000  139270.000000   \n",
       "mean       39.970520      13.990496      18.631141      68.826833   \n",
       "std        13.578201       8.886210       8.798096      19.063650   \n",
       "min         6.000000       0.000000       0.000000       0.000000   \n",
       "25%        31.000000       7.000000      13.000000      57.000000   \n",
       "50%        39.000000      13.000000      19.000000      70.000000   \n",
       "75%        48.000000      19.000000      24.000000      83.000000   \n",
       "max       135.000000     130.000000      87.000000     100.000000   \n",
       "\n",
       "         Humidity3pm    Pressure9am    Pressure3pm      Cloud9am  \\\n",
       "count  137286.000000  127044.000000  127018.000000  88162.000000   \n",
       "mean       51.449288    1017.654577    1015.257963      4.431161   \n",
       "std        20.807310       7.104867       7.035411      2.886594   \n",
       "min         0.000000     980.500000     977.100000      0.000000   \n",
       "25%        37.000000    1013.000000    1010.400000      1.000000   \n",
       "50%        52.000000    1017.600000    1015.200000      5.000000   \n",
       "75%        66.000000    1022.400000    1020.000000      7.000000   \n",
       "max       100.000000    1041.000000    1039.600000      9.000000   \n",
       "\n",
       "           Cloud3pm        Temp9am        Temp3pm  \n",
       "count  84693.000000  140131.000000  138163.000000  \n",
       "mean       4.499250      16.987066      21.693183  \n",
       "std        2.719752       6.496012       6.937784  \n",
       "min        0.000000      -7.200000      -5.400000  \n",
       "25%        2.000000      12.300000      16.600000  \n",
       "50%        5.000000      16.700000      21.100000  \n",
       "75%        7.000000      21.600000      26.400000  \n",
       "max        9.000000      40.200000      46.700000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (streamlit_env)",
   "language": "python",
   "name": "streamlit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
